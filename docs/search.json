[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here I will list some of the projects I have worked on.\n\n\n\n\n\n\nNote\n\n\n\n\n\nI’m still working on transfer my past project to this website. In the meanwhile you can find other projects in:\n. Rpubs\n. Github\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdealista: housing sale in Trieste\n\n\n\n\n\n\nErik De Luca\n\n\nDec 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcGill International Portfolio Challenge\n\n\nInternational competition for the best…\n\n\n\nErik De Luca\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html",
    "title": "McGill International Portfolio Challenge",
    "section": "",
    "text": "International competition for the best portfolio in a fund plan, organized by McGill University, Canada.\nMy aim was to find the best weights that could be used by a pension fund. The portfolio is composed by REITS, Commodities, CAT BOND, Dividends, Short term bonds, Cash and Inflation linked."
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#import-data",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#import-data",
    "title": "McGill International Portfolio Challenge",
    "section": "Import data",
    "text": "Import data\nI built a dataset with tickers, categories and the initial weights of the portfolio.\n\n\nCode\ndetailPortfolio =\n  tibble(\n    tickerList = c(\n      \"SPG\",      # REITS\n      \"OHI\",\n      \"CCOM.TO\",      # Commodities\n      \"DBC\",\n      \"FCX\",\n      \"KSM-F6.TA\",\n      \"0P0001MN8G.F\",      # CAT BOND\n      \"CDZ.TO\",      # dividend\n      \"NOBL\",\n      \"NSRGY\",\n      \"CNI\",\n      \"WFAFY\",\n      \"UU.L\",\n      \"KO\",\n      \"NVS\",\n      \"NVDA\", # nvidia no dividendi\n      # short term bond -- in truth they are etfs that reproduce the trend\n      # \"SHY\",  \n      # \"VGSH\",\n      \"SPTS\",\n      \"IBGS.AS\",\n      # cash -- I placed a Canadian dollar future to represent liquidity\n      \"6C=F\",      \n      \"XGIU.MI\"     # Inflation linked\n    ),\n    category = c(\n      rep(\"REITS\",2),\n      rep(\"Commodities\",4),\n      rep(\"CAT BOND\",1),\n      rep(\"Dividends\",9),\n      rep(\"Short term bonds\",2),\n      rep(\"Cash\",1), \n      rep(\"Inflation linked\",1) \n    ),\n    weight = c(\n      .08,\n      .06,\n      .046,\n      # comodities\n      .009,\n      .01,\n      .057,\n      .07,\n      # cat bond\n      .02,\n      # div\n      .06,\n      .02,\n      .01,\n      .017,\n      .005,\n      .005,\n      .05,\n      .023,\n      # .07,\n      # .07,\n      .05,\n      .128,\n      .15,\n      # .00,\n      .13\n    )\n  )\ndetailPortfolio |&gt; \n  summarise(\n    across(weight, sum, na.rm = TRUE),\n    .by = category\n  ) |&gt; \n  gt() |&gt; \n  fmt_percent(vars(weight), decimals = 1)\n\n\n\n\n\n\n\n\ncategory\nweight\n\n\n\n\nREITS\n14.0%\n\n\nCommodities\n12.2%\n\n\nCAT BOND\n7.0%\n\n\nDividends\n21.0%\n\n\nShort term bonds\n17.8%\n\n\nCash\n15.0%\n\n\nInflation linked\n13.0%\n\n\n\n\n\n\n\n\nFixed Base Indices\nImport data from Yahoo Finance for the last 5 years and construct the portfolio without the weights of each stock.\n\n\nCode\nstockData = lapply(detailPortfolio$tickerList,\n                     getSymbols,\n                       src = \"yahoo\",\n                       from = as.Date(\"2018-09-30\"),\n                       to = as.Date(\"2023-09-29\"),\n                       auto.assign = F\n                     )\nsaveRDS(stockData, \"data/stockData.rds\")\n\n\n\n\nCode\nstockData = readRDS(\"data/stockData.rds\")\n\n# fix tickets that have changed name during import\ndetailPortfolio |&gt; \n  mutate(\n    tickerList = case_when(\n      tickerList == \"KSM-F6.TA\" ~ \"KSM.F6.TA\",\n      tickerList == \"6C=F\" ~ \"X6C\",\n      TRUE ~ tickerList\n    )\n  ) -&gt; detailPortfolio\n\n# compact the different lists\nnominalPortfolio = do.call(merge,stockData) %&gt;% \n  na.omit()\n\n# the CAT BOND is volume-free\nnominalPortfolio$X0P0001MN8G.F.Volume = 1\n\nfor(i in 1:nrow(detailPortfolio))\n{\n  columnSelect = (!names(nominalPortfolio) %like% \"Volume\") &\n    names(nominalPortfolio) %like% detailPortfolio$tickerList[i]\n  nominalPortfolio[,columnSelect] = nominalPortfolio[,columnSelect] / \n    rep(coredata(nominalPortfolio[1,columnSelect])[1],5) \n}\n\nas_tibble(nominalPortfolio)\n\n\n\n  \n\n\n\n\n\nCode\ngrafico = highchart(type = \"stock\")\nfor(i in 1:nrow(detailPortfolio))\n  grafico = hc_add_series(grafico, \n                          Cl(nominalPortfolio[,names(nominalPortfolio) %like%\n                                                detailPortfolio$tickerList[i]]),\n                          name = detailPortfolio$tickerList[i])\ngrafico\n\n\n\n\n\n\n\n\nStock performance in the portfolio\nI add the initial portfolio weights.\n\n\nCode\nportfolio = nominalPortfolio\n\nfor(i in 1:nrow(detailPortfolio))\n{\n  columnSelect = (!names(portfolio) %like% \"Volume\") &\n    names(portfolio) %like% detailPortfolio$tickerList[i]\n  portfolio[,columnSelect] = \n    coredata(nominalPortfolio[,columnSelect]) * detailPortfolio$weight[i] \n}\n\nas_tibble(portfolio)\n\n\n\n  \n\n\n\n\n\nCode\ngrafico = highchart(type = \"stock\")\nfor(i in 1:nrow(detailPortfolio))\n  \n  grafico = hc_add_series(grafico, \n                          Cl(portfolio[,names(portfolio) %like% \n                                         detailPortfolio$tickerList[i]]),\n                          name = detailPortfolio$tickerList[i]) \ngrafico\n\n\n\n\n\n\n\n\nPortfolio Creation\nI create the portfolio by adding up the indices of the stocks multiplied by their weights. This gives the overall performance of the portfolio.\n\n\nCode\ncolumnNames = c(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adjusted\")\nmyPortfolio = matrix(NA, nrow(portfolio),ncol = length(columnNames))\n\nfor(i in 1:length(columnNames))\n{\n  columnSelect = names(portfolio) %like% columnNames[i]\n  myPortfolio[,i] = sapply(1:nrow(portfolio), \n                           function(r) sum(coredata(portfolio[r,columnSelect])))  \n}\n\ncolnames(myPortfolio) = paste(\"Portfolio\", columnNames, sep = \".\")\nmyPortfolio = xts(myPortfolio, order.by = index(portfolio))\n\nas_tibble(myPortfolio)\n\n\n\n  \n\n\n\n\n\nCode\np = myPortfolio %&gt;% \n  fortify() %&gt;% \n  ggplot(aes(x = Index, y = Portfolio.Open)) + \n  geom_smooth(method = \"gam\",\n              formula = formula(y ~ s(x)),\n              fill = pal[5],\n              aes(color = pal[5]),\n              alpha = .3) +\n  geom_line(color = pal[3]) +\n  geom_pointrange(aes(ymin = Portfolio.Low, \n                    ymax = Portfolio.High), \n              alpha = 0.22,\n              fill =  'turquoise1',\n              size = .1) +\n  # geom_col(aes(y = Portfolio.Volume),inherit.aes = F) +\n  xlab(\"\") +\n  ylab(\"\") +\n  scale_y_continuous(labels = function(x) scales::percent(x-1)) +\n  scale_color_manual(values = pal[5],\n                     labels = \"Prediction of portfolio trends using splines\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\np\n\n\n\n\n\n\n\n\n\nCode\n# ggplotly(p)"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#decomposition-historical-series",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#decomposition-historical-series",
    "title": "McGill International Portfolio Challenge",
    "section": "Decomposition historical series",
    "text": "Decomposition historical series\nI decompose the historical series to display its different components.\n\n\nCode\npfDecomposto = decompose(ts(myPortfolio$Portfolio.Open %&gt;% as.vector(),\n                            start = c(2022, 9, 29),\n                            # end = c(2023, 09, 28),\n                            frequency = 7))\n# plot(pfDecomposto)\np = tibble(Dates = seq(as.Date(\"2022-09-29\"),\n                   length = length(pfDecomposto$x),\n                   by =  \"days\"),\n       Serie = pfDecomposto$x %&gt;% coredata(),\n       Seasonal = pfDecomposto$seasonal %&gt;% coredata(),\n       Trend = pfDecomposto$trend %&gt;% coredata(),\n       Random = pfDecomposto$random %&gt;% coredata()) %&gt;% \n  gather(key = \"Type\", -Dates, value = \"y\") %&gt;% \n  ggplot(aes(x = Dates, y = y)) + \n    geom_line() +\n    facet_grid(rows = vars(Type), \n               scales = \"free_y\") +\n  ylab(\"\")  +\n  scale_x_date(date_breaks = \"1 month\",\n               date_labels = \"%b\")\n\nggplotly(p, dynamicTicks = TRUE) %&gt;%\n  # rangeslider() \n  plotly::layout(hovermode = \"x\")"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#optimisation",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#optimisation",
    "title": "McGill International Portfolio Challenge",
    "section": "Optimisation",
    "text": "Optimisation\nThe current portfolio has these indices, our goal now is to optimise the portfolio to maximise the sharpe rato index, i.e. to maximise the portfolio’s return while minimising its risk.\n\n\nCode\nnominalPortfolioAdj = \n  nominalPortfolio[,names(nominalPortfolio) %like% \"Adjusted\"] %&gt;%\n  CalculateReturns() %&gt;% \n  to.yearly.contributions() %&gt;% \n  na.omit()\n\nweight = detailPortfolio$weight\n\nnominalPortfolioAdj = \n  nominalPortfolioAdj[,names(nominalPortfolioAdj) != \"Portfolio Return\"]\n\nmean_ret = colMeans(nominalPortfolioAdj)\n\ncov_mat = nominalPortfolio[,names(nominalPortfolio) %like% \"Adjusted\"] %&gt;%\n  CalculateReturns() %&gt;% \n  to.quarterly.contributions() %&gt;% \n  na.omit() %&gt;% \n  cov()\n\n# solo quando i volumi non sono degeneri\n\nreturn3mesi = nominalPortfolio[,!names(nominalPortfolio) %like% \"Volume\"] %&gt;%\n  CalculateReturns() %&gt;%\n  to.period.contributions(\"quarters\") %&gt;%\n  na.omit()\n\nvar3m = VaR(R = return3mesi[,names(return3mesi) %like% \"Adjusted\"],\n            method = \"historical\",\n            portfolio_method = \"component\",\n            weights = weight)\n\nport_risk = var3m$hVaR\n\ncov_mat = cov_mat[rownames(cov_mat) != \"Portfolio Return\",\n                  colnames(cov_mat) != \"Portfolio Return\"]\n\nport_returns = sum(mean_ret * weight)\n\nport_risk = sqrt(t(weight) %*% (cov_mat %*% weight))\n\nsharpe_ratio = port_returns/port_risk\n\ntibble(\"Return\" = port_returns,\n       \"Risk\" = port_risk,\n       \"VaR a 3 mesi\" = var3m$hVaR,\n       \"Sharpe ratio\" = sharpe_ratio) |&gt; \n  gt() |&gt; \n  fmt_percent(columns = 1:3) |&gt; \n  fmt_number(columns = 4, decimals = 2)\n\n\n\n\n\n\n\n\nReturn\nRisk\nVaR a 3 mesi\nSharpe ratio\n\n\n\n\n6.77%\n2.28%\n−0.96%\n2.98\n\n\n\n\n\n\n\n\nMonteCarlo Simulation\nA simulation will be carried out using the MonteCarlo method where the experiment will be repeated by randomly extracting the weights, thus finding the best portfolio combination. In this case, the experiment will be repeated 5000 times but the portfolio weights will not be completely random, they will vary around the values we preset.\n\n\nCode\nset.seed(1)\nnum_port = 5000\n\n# Creating a matrix to store the weights\nall_wts = matrix(nrow = num_port,\n                  ncol = nrow(detailPortfolio))\n\n# Creating an empty vector to store\n# Portfolio returns\nport_returns = vector('numeric', length = num_port)\n\n# Creating an empty vector to store\n# Portfolio Standard deviation\nport_risk = vector('numeric', length = num_port)\n\n# Creating an empty vector to store\n# Portfolio Sharpe Ratio\nsharpe_ratio = vector('numeric', length = num_port)\n\nfor (i in seq_along(port_returns)) {\n  precisione = 0.9\n  wts = sapply(1:length(weight), \n               function(i) runif(1,\n                                 precisione * weight[i],\n                                 (2 - precisione) * weight[i]))\n  # wts = runif(length(tickerList))\n  wts = wts/sum(wts)\n  \n  # Storing weight in the matrix\n  all_wts[i,] = wts\n  \n  # Portfolio returns\n  \n  port_ret = sum(wts * mean_ret)\n  # port_ret &lt;- ((port_ret + 1)^252) - 1\n  \n  # Storing Portfolio Returns values\n  port_returns[i] = port_ret\n  \n  \n  # Creating and storing portfolio risk\n  port_sd = sqrt(t(wts) %*% (cov_mat  %*% wts))\n\n  # Più preciso ma ci mette troppo  \n  # port_sd = VaR(\n  #   R = return3mesi[, names(return3mesi) %like% \"Adjusted\"],\n  #   method = \"historical\",\n  #   portfolio_method = \"component\",\n  #   weights = wts\n  # )$hVaR\n\n  port_risk[i] = port_sd\n  \n  \n  # Creating and storing Portfolio Sharpe Ratios\n  # Assuming 0% Risk free rate\n  sr = port_ret/port_sd\n  sharpe_ratio[i] = sr\n}\n\n\n\n\nPortfolio weights\nThe weights assigned by the portfolio with the highest sharpe ratio are shown in the interactive plot below.\n\n\nCode\n# Storing the values in the table\nportfolio_values = tibble(Return = port_returns,\n                  Risk = port_risk,\n                  SharpeRatio = sharpe_ratio)\n\n\n# Converting matrix to a tibble and changing column names\nall_wts = all_wts %&gt;%\n  data.frame() %&gt;%\n  tibble\ncolnames(all_wts) = detailPortfolio$tickerList\n\n# Combing all the values together\nportfolio_values = tibble(cbind(all_wts, portfolio_values))\ncolnames(portfolio_values)[1:nrow(detailPortfolio)] = detailPortfolio$tickerList\n\nmin_var = portfolio_values[which.min(portfolio_values$Risk),]\nmax_sr = portfolio_values[which.max(portfolio_values$SharpeRatio),]\n\n# weightOLD = weight\nweight = max_sr[,1:nrow(detailPortfolio)] %&gt;% \n  as.numeric() %&gt;% \n  round(4) \n\n# con l'arrotondamento potrebbe non fare 1 e lo calibro con il primo titolo, \n# ciò non influenzerà significativamente sullo scostamento del portafoglio\nweight[1] = weight[1] + 1 - sum(weight)\n\n# max_sr %&gt;% \n#   t() %&gt;%\n#   data.frame()\n\np = max_sr %&gt;%\n  gather(detailPortfolio$tickerList, key = Asset,\n         value = Weights) %&gt;%\n  cbind(Category = factor(detailPortfolio$category)) %&gt;% \n  mutate(Asset = Asset %&gt;%\n           as.factor() %&gt;% \n           fct_reorder(Weights),\n         Percentage = str_c(round(Weights*100,2), \"%\")) %&gt;%\n  ggplot(aes(x = Asset,\n             y = Weights,\n             fill = Category,\n             label = Percentage)) +\n  geom_bar(stat = 'identity') +\n  geom_label(nudge_y = .01, size = 3) +\n  theme_minimal() +\n  labs(x = 'Tickers',\n       y = 'Weights',\n       title = \"Weights of the portfolio tangent to the efficient frontier\") +\n  scale_y_continuous(labels = scales::percent) +\n  guides(fill = guide_legend(override.aes = aes(label = \"\"))) + \n  theme(legend.position = \"bottom\") +\n  coord_flip()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nEfficient frontier\nThe graph below shows the values of the portfolios created during the optimisation process. The red dot represents the portfolio with the highest sharpe ratio.\n\n\nCode\np = portfolio_values %&gt;%\n  ggplot(aes(x = Risk, y = Return, color = SharpeRatio)) +\n  geom_point() +\n  theme_classic() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = 'Annual risk',\n       y = 'Annual return',\n       title = \"Portfolio optimization and efficient frontier\") +\n  geom_point(aes(x = Risk,\n                 y = Return),\n             data = max_sr,\n             color = 'darkred') \n\nggplotly(p)"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#optimised-portfolio",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#optimised-portfolio",
    "title": "McGill International Portfolio Challenge",
    "section": "Optimised portfolio",
    "text": "Optimised portfolio\n\n\nCode\n# I recreate the portfolio with the new weights\nportfolio = nominalPortfolio\n\nfor(i in 1:nrow(detailPortfolio))\n{\n  columnSelect = (!names(portfolio) %like% \"Volume\") & \n    names(portfolio) %like% detailPortfolio$tickerList[i]\n  portfolio[,columnSelect] = \n    coredata(nominalPortfolio[,columnSelect]) * weight[i] \n}\n\ncolumnNames = c(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adjusted\")\nmyPortfolio = matrix(NA, nrow(portfolio),ncol = length(columnNames))\n\nfor(i in 1:length(columnNames))\n{\n  columnSelect = names(portfolio) %like% columnNames[i]\n  myPortfolio[,i] = sapply(\n    1:nrow(portfolio), \n    function(r) sum(coredata(portfolio[r,columnSelect])))  \n}\n\ncolnames(myPortfolio) = paste(\"Portfolio\", columnNames, sep = \".\")\nmyPortfolio = xts(myPortfolio, order.by = index(portfolio)) \n\nmyPortfolio %&gt;% \n  fortify() %&gt;% \n  mutate(across(starts_with(\"Portfolio\"), \\(x) x/dplyr::first(x))) %&gt;%\n  ggplot(aes(x = Index, y = Portfolio.Open)) + \n  geom_smooth(method = \"gam\",\n              formula = formula(y ~ s(x)),\n              fill = pal[5],\n              aes(color = pal[5]),\n              alpha = .3) +\n  geom_line(color = pal[3]) +\n  geom_pointrange(aes(ymin = Portfolio.Low, \n                      ymax = Portfolio.High), \n              alpha = 0.22,\n              fill = 'turquoise1',\n              size = .1) +\n  # geom_col(aes(y = Portfolio.Volume),inherit.aes = F) +\n  xlab(\"\") +\n  ylab(\"\") +\n  scale_y_continuous(labels = function(x) scales::percent(x)) +\n  scale_color_manual(values = pal[5],\n                     labels = \"Prediction of portfolio trends using splines\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#var-value-at-risk",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#var-value-at-risk",
    "title": "McGill International Portfolio Challenge",
    "section": "VaR: Value at Risk",
    "text": "VaR: Value at Risk\nIn the following graph, the different securities are shown with their yield, their variance on the abscissa and are coloured according to their coefficient of variation. This graph helps in the choice of initial weights (pre-optimisation) as it is easy to visualise those that perform best.\n\n\nCode\nreturnTicker = \n  map_dfc(\n    detailPortfolio$tickerList,\n    ~dailyReturn(Cl(nominalPortfolio[,names(portfolio) %like% .x]))\n  )\ncolnames(returnTicker) = detailPortfolio$tickerList\n\nreturnTickerIndici = returnTicker %&gt;% \n  as.tibble() %&gt;%\n  summarise_all(sum) %&gt;% \n  pivot_longer(\n    1:nrow(detailPortfolio),\n    names_to = \"Titoli\",\n    values_to = \"Rendimento\"\n    )  %&gt;% \n  add_column(returnTicker %&gt;%\n               as.tibble() %&gt;%\n               summarise_all(sd) %&gt;%\n               pivot_longer(\n                 1:nrow(detailPortfolio),\n                 names_to = \"Titoli\",\n                 values_to = \"Varianza\"\n                 ) %&gt;% \n               dplyr::select(Varianza)\n             ) %&gt;% \n  mutate(Variazione = ifelse(round(Rendimento, 2) != 0,\n                             Varianza/abs(Rendimento),\n                             1)) \n\nhValMin = 1.8 # giocando con questo parametro si cambia l'asse delle y\n# modificando la distanza dei punti estremi ai punti centrali\nhValMax = 1\n\np = returnTickerIndici %&gt;%\n  mutate(quantili = punif(Rendimento,\n                       min = hValMin * min(Rendimento), # se non metto hvalmin\n                       # il min di Rendimento vale 0 e di conseguenza il log\n                       # tende a meno infinito\n                       max = hValMax * max(Rendimento),\n                       log.p = T), \n    across(where(is.numeric), \\(x) round(x, 4)),\n    # across(vars(Rendimento), )\n  ) |&gt; \n  ggplot(aes(y = quantili,\n             x = Varianza,\n             color = Variazione,\n             label = Titoli,\n             z = Rendimento #serve solo per l'etichetta nel grafico interattivo\n             )) +\n  geom_point(size = 1.5) + \n  scale_color_distiller(palette = \"RdYlGn\", direction = -1) +\n  scale_x_log10(labels = scales::percent_format(accuracy = .2),\n                breaks = scales::breaks_log(n = 10, base = 10)) +\n  scale_y_continuous(\n    labels = function(x) scales::percent(\n      qunif(x,\n            min = hValMin * min(returnTickerIndici$Rendimento),\n            max = hValMax * max(returnTickerIndici$Rendimento),\n            # associo i valori originali invertendo la funzione di ripartizione\n            log.p = T), \n    scale = 1\n    ),\n                     breaks = scales::pretty_breaks(10)) +\n  labs(x = \"Variation\", y = \"Return\", color = \"Coefficient \\nof variation\") +\n  theme(legend.position = \"right\", \n        legend.title.align = 0) \n\n\nggplotly(p, tooltip = c(\"z\", \"x\", \"color\", \"label\"))\n\n\n\n\n\n\n\n\nCode\nreturn3mesi = nominalPortfolio %&gt;% \n  CalculateReturns %&gt;% \n  to.period.contributions(\"quarters\")\n\n\nweight_max_sr = max_sr %&gt;% \n      t() %&gt;% \n      head(nrow(detailPortfolio)) %&gt;% \n      as.vector()\n\nVaR(return3mesi[,names(return3mesi) %like% \"Open\"],\n    method = \"historical\",\n    weights = weight_max_sr,\n    portfolio_method = \"marginal\") %&gt;% \n  pivot_longer(1:length(weight_max_sr) + 1,\n               names_to = \"Titoli\",\n               values_to = \"VaR\") %&gt;%\n  mutate(VaR = round(VaR *100, 2)) %&gt;% \n  ggplot(aes(x = Titoli, y = VaR, fill = VaR)) +\n  geom_col() +\n  geom_hline(aes(yintercept = PortfolioVaR), color = \"orchid\") +\n  coord_flip() + \n  scale_fill_distiller(palette = \"RdYlGn\", direction = 1) +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = function(x) str_remove(x,\".Open\")) +\n  labs(\n    x = \"\",\n    title = \"Value at Risk of the single securities\",\n    ) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = scales::pretty_breaks(8))\n\n\n\n\n\n\n\n\n\nThe following histogram shows the simulation of 1000000 samples taken from a normal of mean equal to the portfolio return on a four-monthly basis and the variance equal to the portfolio variance on a four-monthly basis.\n\n\nCode\nalpha = 0.005\n\nmedia = sapply(1:nrow(return3mesi), function(i)\n  sum(return3mesi[i, names(nominalPortfolio) %like% \"Adjusted\"]\n      * weight_max_sr)) %&gt;%\n  mean(na.rm = T)\n\nvarianza = sapply(1:nrow(return3mesi), function(i)\n  sum(return3mesi[i,names(nominalPortfolio) %like% \"Adjusted\"]\n      * weight_max_sr)) %&gt;%\n  sd(na.rm = T)\n\nset.seed(1)\ndf = data.frame(x = rnorm(1E6, media, varianza))\nggplot(df, aes(x, ..density..)) +\n  geom_histogram(color = \"violet\",\n                 fill = \"orchid1\",\n                 alpha = .5,\n                 bins = 30) +\n  geom_density(color = \"aquamarine\") +\n  geom_vline(xintercept = quantile(df$x, probs = alpha),\n             color = \"aquamarine2\") +\n  annotate('text',\n           x = quantile(df$x, probs = alpha),\n           y = 0.01,\n           color = \"aquamarine4\",\n           label = paste(\"VaR = \",df$x %&gt;% \n                           quantile(probs = alpha) %&gt;% \n                           round(4))) +\n  scale_y_continuous(labels = NULL) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Value at Risk\",\n    )"
  },
  {
    "objectID": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#correlation",
    "href": "projects/2023-10-15_MIPC_Asset_Allocator/index.html#correlation",
    "title": "McGill International Portfolio Challenge",
    "section": "Correlation",
    "text": "Correlation\nThe correlation chart is useful to see the diversification of the portfolio.The plot is interactive, so you can zoom in and out to see the details.\n\n\nCode\ncorrelazione = return3mesi[,names(return3mesi) %like% \"Adjusted\"] %&gt;% \n  na.omit %&gt;% \n  cor\n\ncolnames(correlazione) = stringr::str_remove(colnames(correlazione),\".Adjusted\")\nrownames(correlazione) = stringr::str_remove(colnames(correlazione),\".Adjusted\")\n\n# Funzione personalizzata per etichette colorate\ncolor_labels = function(labels, colors) {\n  mapply(function(label, color) {\n    as.expression(bquote(bold(.(color)(.(label)))))\n  }, labels, colors, SIMPLIFY = FALSE)\n}\n\np = correlazione %&gt;% \n  reshape2::melt() %&gt;% \n  ggplot(aes(x=Var1, y=Var2, fill = value, color = value)) + \n  geom_tile() +\n  scale_fill_distiller(name = \"Correlation\",\n                       palette = \"RdYlGn\",\n                       direction = 1) +\n  # geom_label(aes(label = round(value,2)), size =2,label.size = 0) +\n  scale_x_discrete(limits = rev(rownames(correlazione))) +\n  # Imposta color su NULL per nasconderlo\n  guides(color = guide_legend(override.aes = list(color = NULL))) +  \n  theme(axis.title = element_blank(),\n        axis.text.x = element_text(angle = 30,vjust = .95, hjust = .95))\n\nggplotly(p)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erik De Luca",
    "section": "",
    "text": "Hello! I’m Erik De Luca, an actuary with a passion for data science, always seeking new ways to transform complex numbers and data into comprehensible stories and practical solutions. With a background in statistics and actuarial techniques, I have a keen interest in quantitative finance and portfolio optimization. When not immersed in numbers, I indulge in travel, craft beer brewing, and judo. Welcome to my corner of the web!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Here is a resume of my education and working experiences.\n\n\nSince I started the master I have been working jobs related to my field of studies. Before, I worked in different backgrounds. Thanks to the multifacetic jobs that I had, I was able to expand my vision.\n\n\nI just came back from Munich (Germany) where I completed my internship at Allianz, one of the world’s leading insurance and financial services providers. I worked for 6 months at Allianz SE, holding company of Allianz Group, in the H2 department, related to the finance processes. Specifically, I worked in the Group Actuarial Non-Life team, responsible for the Actuarial Function of the company, which is the team in charge of risk assessment, financial modeling, regulatory compliance, and strategic planning.\nIn my case, I was dedicated to:\n\nContributing to the team’s reporting products. Mainly Inflation Reserve and Group Reserve Committee;\nTesting new Reserve Reviewing approaches in ResQ, in particular, Bootstrapping for DFMs and Stochastic Result Selection to deal with confidence intervals for the ultimate claims;\nWorking on an internal project aiming to build and disclose a new model, developed in R, in order to properly assess implicit inflation in Run-Off triangles; in collaboration with one of the companies of the Group;\nSupport in building tools to facilitate and develop day-to-day processes;\nAutomatizing core reporting processes, by bridging data flow from SAP, Excel, and Word Implementation in VBA.\n\n\n\n\nFrom August 2023 to December 2024, I was involved in national medical research where I was in charge of the statistical part. The topic was “Education in lower and upper secondary schools and support of the network of reference figures for youth for prevention of HPV and other sexually transmitted infections”.\nMy tasks were:\n\nAnalyzing surveys via R and building reports for the different geographic areas and school levels;\nBuilding reports which contained stratified frequency tables, charts, models and statistical tests;\nWriting the methodologic analysis in research papers.\n\nThe models built were GLM (generalized linear models), some of them were zero-inflated quasipoisson due to the scoring rules of the survey. The statistical tests used were:\n\nMcNemar test for detecting if there is a change between the pre and post intervention.\nKruskal-Wallis test to identify if one socio-demographic characteristic is stochasticity superior to another one. This is also confirmed by the terms of the models built.\nChi-square and proportion test to inference when the frequency of an answer is different by socio-demographic characteristics.\n\nThe project is still on the way and once it finish, I’ll upload here all my notebooks and link my papers.\n\n\n\nIt sounds rare but during my bachelor’s, I worked for almost 3 years as a teacher in a public primary school. I started during the pandemic when there was a huge lack of teachers and I took the opportunity!\nIt was a big challenge for me. I was in charge of teaching subjects such as math and science to children from 6 to 11 y.o. It taught me to manage a team of about 25 people (the students), very heterogeneous. Actually, I didn’t learn only how to manage students, but also the other class teachers, as I was given the role of coordinator, handling all the bureaucratic stuff.\nIt was a really important part of my life where I learned (not only taught) a lot of soft skills about explaining hard concepts in an easy way (for the children is hard, trust me) and managing teams up to 25 people (and sometimes even parents ). In the meanwhile, I continued my studies so strategic planning of my life was the real key to success.\n\n\n\n\nHere is a short overall about my studies:\n\nMaster’s degree in Statistics and Actuarial Science with a curriculum in Data Science for Finance and Insurance at the University of Trieste. (Currently enrolled)\nErasmus, during my master, at the University of Malaga (Spain) where the classes were in Spanish.\nBachelor’s degree in Statistics and IT for Business, Finance, and Insurance, my thesis was focused on decisional tree models applied to the brewery industry.\n\nSo, my fields of study are mainly statistics, data science, actuarial science, IT, and finance."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About me",
    "section": "",
    "text": "Here is a resume of my education and working experiences.\n\n\nSince I started the master I have been working jobs related to my field of studies. Before, I worked in different backgrounds. Thanks to the multifacetic jobs that I had, I was able to expand my vision.\n\n\nI just came back from Munich (Germany) where I completed my internship at Allianz, one of the world’s leading insurance and financial services providers. I worked for 6 months at Allianz SE, holding company of Allianz Group, in the H2 department, related to the finance processes. Specifically, I worked in the Group Actuarial Non-Life team, responsible for the Actuarial Function of the company, which is the team in charge of risk assessment, financial modeling, regulatory compliance, and strategic planning.\nIn my case, I was dedicated to:\n\nContributing to the team’s reporting products. Mainly Inflation Reserve and Group Reserve Committee;\nTesting new Reserve Reviewing approaches in ResQ, in particular, Bootstrapping for DFMs and Stochastic Result Selection to deal with confidence intervals for the ultimate claims;\nWorking on an internal project aiming to build and disclose a new model, developed in R, in order to properly assess implicit inflation in Run-Off triangles; in collaboration with one of the companies of the Group;\nSupport in building tools to facilitate and develop day-to-day processes;\nAutomatizing core reporting processes, by bridging data flow from SAP, Excel, and Word Implementation in VBA.\n\n\n\n\nFrom August 2023 to December 2024, I was involved in national medical research where I was in charge of the statistical part. The topic was “Education in lower and upper secondary schools and support of the network of reference figures for youth for prevention of HPV and other sexually transmitted infections”.\nMy tasks were:\n\nAnalyzing surveys via R and building reports for the different geographic areas and school levels;\nBuilding reports which contained stratified frequency tables, charts, models and statistical tests;\nWriting the methodologic analysis in research papers.\n\nThe models built were GLM (generalized linear models), some of them were zero-inflated quasipoisson due to the scoring rules of the survey. The statistical tests used were:\n\nMcNemar test for detecting if there is a change between the pre and post intervention.\nKruskal-Wallis test to identify if one socio-demographic characteristic is stochasticity superior to another one. This is also confirmed by the terms of the models built.\nChi-square and proportion test to inference when the frequency of an answer is different by socio-demographic characteristics.\n\nThe project is still on the way and once it finish, I’ll upload here all my notebooks and link my papers.\n\n\n\nIt sounds rare but during my bachelor’s, I worked for almost 3 years as a teacher in a public primary school. I started during the pandemic when there was a huge lack of teachers and I took the opportunity!\nIt was a big challenge for me. I was in charge of teaching subjects such as math and science to children from 6 to 11 y.o. It taught me to manage a team of about 25 people (the students), very heterogeneous. Actually, I didn’t learn only how to manage students, but also the other class teachers, as I was given the role of coordinator, handling all the bureaucratic stuff.\nIt was a really important part of my life where I learned (not only taught) a lot of soft skills about explaining hard concepts in an easy way (for the children is hard, trust me) and managing teams up to 25 people (and sometimes even parents ). In the meanwhile, I continued my studies so strategic planning of my life was the real key to success.\n\n\n\n\nHere is a short overall about my studies:\n\nMaster’s degree in Statistics and Actuarial Science with a curriculum in Data Science for Finance and Insurance at the University of Trieste. (Currently enrolled)\nErasmus, during my master, at the University of Malaga (Spain) where the classes were in Spanish.\nBachelor’s degree in Statistics and IT for Business, Finance, and Insurance, my thesis was focused on decisional tree models applied to the brewery industry.\n\nSo, my fields of study are mainly statistics, data science, actuarial science, IT, and finance."
  },
  {
    "objectID": "about.html#personal-life",
    "href": "about.html#personal-life",
    "title": "About me",
    "section": "Personal Life",
    "text": "Personal Life\n\nSport\nI think that sport is very important in the life and helps to develop strength and health. Main sports:\n\nJudo: From 4 y.o., I have done international competitions around Europe. In 2016, I went for 3 weeks for a training camp in Japan. But about Judo, I’m proud of my siblings who have achieved world results, as my brother was 3rd in the World Judo Championship and 2nd in the European Judo Championship keeping the 1st place in the Juniors (U21) ranking -66Kg.\nRun: I’m not very fast and not the best runner definitely. But I’m really proud about a personal challenge done in 2023, where I finished a trail of 57 Km without ever having ran more than 21 Km. It was quite mind-challenging.\nMy secret dream is climbing, and that is my scope for the future. Until now I have only practiced bouldering in Germany, which is a fantastic place for that., but for sure one of my purposes for 2025 is finding my new place for bouldering.\n\n\n\nTravel\nI could be boring on this point but it is the truth. I think that post-COVID most people changed their habits and I wasn’t the exception. I’ve been to 20 countries, but some of them have been important for me:\n\nColombia: Follow my love and discover another one. That is the magic about travelling. I’ve lived for 3 months around this amazing country really different from what I expected.\nMalaga (Spain): I’m sure that everyone who has done an exchange program (e.g. Erasmus) can agree with me, I Ieft a piece of my heart.\nSweden: It took place in my first Erasmus project and helped me to fight my fear for English, besides that I got the chance of meeting people from around Europe full of energy and positivity.\nMarocco: I’ve been twice but why talking about the easy time with a touristic tour? Well, my girlfriend and I decided to take a random flight to the most unknown city, Nador, and what an adventure with no data and no local language during Ramadan. It has been a full immersion in the Arabic culture.\nHouston (USA): I went to a wedding and I had the opportunity to live as a Texan for one week.\n\n\n\nBeer\nI want to talk about the real beer: the craft one. I’m a nerd about craft beer, I brew my beer since 10 years ago and I use the BIAP method (Brew In A Pot). I love this hobby because it is a mix of creativity, science, and DIY. But I don’t only make it, I also taste it and as a good statistician, I record each one on Untapped!"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Erik De Luca",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public: \nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "projects/2024-12-17_Idealista_API/index.html",
    "href": "projects/2024-12-17_Idealista_API/index.html",
    "title": "Idealista: housing sale in Trieste",
    "section": "",
    "text": "The aim of this project is to download all the ads of houses for sale in Trieste from the Idealista website and conduct a preliminary analysis of the data. Moreover, we will create a map with the location of the houses for sale, to get an idea of the distribution of the ads and their prices in the city."
  },
  {
    "objectID": "projects/2024-12-17_Idealista_API/index.html#get-the-data",
    "href": "projects/2024-12-17_Idealista_API/index.html#get-the-data",
    "title": "Idealista: housing sale in Trieste",
    "section": "Get the Data",
    "text": "Get the Data\n\n\nCode\nlibrerie = c(\n  \"jsonlite\",\n  \"httr\", # for the API\n  \"scales\",\n  \"ggplot2\",\n  \"leaflet\", # for the map\n  \"RColorBrewer\", # for the map\n  \"doParallel\",\n  \"parallel\",\n  # \"reticulate\", \n  \"tidymodels\",\n  \"tidyverse\",\n  \"plotly\",\n  \"randomForest\" # for na.roughfix\n)\n\nInstall_And_Load &lt;- function(packages) {\n  k &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])];\n  if(length(k))\n  {install.packages(k, repos='https://cran.rstudio.com/');}\n\n  for(package_name in packages)\n  {library(package_name,character.only=TRUE, quietly = TRUE);}\n}\n\n# the tidyverse functions are used instead of the others in case of same name\n# for two functions\n# conflicted::conflict_prefer_all(\"tidyverse\")\nconflicted::conflict_prefer_all(\"dplyr\")\nconflicted::conflict_prefer_all(\"ggplot2\")\n\n\nInstall_And_Load(librerie)\n\ntheme_set(theme_minimal())\n\n\nTo access the idealista APIs we need to obtain the credentials. You can get them by registering on the idealista website.\n\n\nCode\n# parametri in input\n\n# nuove credenziali\n\nconsumer_key = readRDS(\"keys/consumer_key.rds\")\nconsumer_secret = readRDS(\"keys/consumer_secret.rds\")\n# saveRDS(consumer_key, file = \"keys/consumer_key.rds\")\n# saveRDS(consumer_secret, file = \"keys/consumer_secret.rds\")\n\n\n#Use basic authentication\nsecret &lt;- jsonlite::base64_enc(paste(consumer_key, consumer_secret, sep = \":\"))\nreq &lt;- httr::POST(\"https://api.idealista.com/oauth/token\",\n                  httr::add_headers(\n                    #\"Authorization\" = paste(\"Basic\", gsub(\"n\", \"\", secret)),\n                    \"Authorization\" = paste(\"Basic\", secret, sep = \" \"),\n                    \"Content-Type\" = \n                      \"application/x-www-form-urlencoded;charset=utf-8\"\n                  ),\n                  body = \"grant_type=client_credentials\"\n)\n\ntoken &lt;- paste(\"Bearer\", httr::content(req)$access_token)\n\n\nEstablish the parameters for the request link. Our goal is to obtain all ads for houses for sale in Trieste. The central point is set to the center of Trieste, with a maximum distance of 10 km. We also require a minimum size of 30 square meters for the ads Establish the parameters for the request link. Our goal is to obtain all ads for houses for sale in Trieste. The central point is set to the center of Trieste, with a maximum distance of 10 km. We also require a minimum size of 30 square meters for the ads to exclude garages.to exclude garages.\n\n\nCode\n#url user parameters\n# x = '36.71145256718129' For Malaga\n# y = '-4.4288958904720355'\nx = '45.643170'\ny = '13.790524'\nmaxItems = '10000'\ndistance = '10000'\ntype = 'homes'\nop = 'sale'\nminprice = '30001'\nmaxprice = '200000000'\nminsize = '30'\nmaxsize = '10000'\n\n\n#url fixed parameters\n# site = 'https://api.idealista.com/3.5/es/search?' For Spain\nsite = 'https://api.idealista.com/3.5/it/search?'\nloc = 'center='\n# country = '&country=es'\ncountry = '&country=it'\nmaxitems = '&maxItems=50'\npages = '&numPage='\ndist = '&distance='\nproperty = '&propertyType='\noperation = '&operation='\npricefrom = '&minPrice='\npriceto = '&maxPrice='\nmisize = '&minSize='\nmasize = '&maxSize='\nchalet = '&chalet=0'\n\n\nNow, we will send the request to Idealista. We have a monthly limit of 100(pagina = 100) for requests and one for second (Sys.sleep(1)). Each request is different only by the result page’s index.\nOnce the data are downloaded and extracted from JSON, we’ll get the lists that have to extract and put in a dataset. The problem araises becuase inside the lists are present other lists nested and not for each ad but only for someone. So that we create an empty matrix with the unique items number for ads as columns and the number of rows equal to the number of ads.\n\n\nCode\npagina = 100\n\nfor(z in 1:pagina)\n{\n  print(z)\n  \n  # prepara l'url\n  url &lt;- paste0(site, loc, x, ',', y, country, maxitems, pages, z, dist, distance,\n               property, type, operation, op, pricefrom, minprice, priceto, maxprice,\n               misize, minsize, masize, maxsize)\n  \n  # invia la richiesta a idealista\n  res &lt;- httr::POST(url, httr::add_headers(\"Authorization\" = token))\n  \n  # estrai il contenuto dal JSON \n  cont_raw &lt;- httr::content(res) \n  \n  # stop the cycle if there are no more results\n  if(length(cont_raw[[1]]) == 0) break\n\n    # NUOVO: Prendo i nomi delle colonne da tutte le liste e li unisco\n  map(\n    1:length(cont_raw[[1]]),\n    function(x) {\n      # the if is necessary because the list can be empty in the last page\n      if(length(cont_raw[[1]][[x]]) == 0) return(NULL)\n      return(names(cont_raw[[1]][[x]]))\n    }\n  ) |&gt; \n    unlist() |&gt;\n    unique() -&gt; colNames\n  \n  # Creo una matrice vuota dove imagazzinare i valori\n  m = matrix(NA, nrow = length(cont_raw[[1]]), ncol = length(colNames))\n  colnames(m) = colNames\n  for(r in 1:length(cont_raw[[1]]))\n  {\n    for(c in 1:length(cont_raw[[1]][[r]]))\n    {\n      # nel caso l'elemento della lista sia una sotto lista o df vado a \n      # spacchettarlo aggiungendo colonne\n      if(length(cont_raw[[1]][[r]][[c]])&gt;1)\n      {\n        # non si può fare in un unico caso\n        for(i in 1:length(cont_raw[[1]][[r]][[c]]))\n        {\n          # se la colonna della sottolista non è già stata aggiunta lo faccio\n          if(is.null(names(cont_raw[[1]][[r]][[c]])))\n          {\n            cont_raw[[1]][[r]][[c]] = cont_raw[[1]][[r]][[c]][[1]] \n          }\n          if(!names(cont_raw[[1]][[r]][[c]])[i] %in% colNames)\n          {\n            colNames = c(colNames, names(cont_raw[[1]][[r]][[c]])[i])\n            m = cbind(m, rep(NA,length(cont_raw[[1]]))) # aggiunta della colonna\n            colnames(m) = colNames\n          }\n        }\n        # inserisco i dati della sottolista\n        for(k in 1:length(cont_raw[[1]][[r]][[c]]))\n          m[r,names(cont_raw[[1]][[r]][[c]])[k]] = cont_raw[[1]][[r]][[c]][[k]]\n      }else{\n        tryCatch(\n          {\n            m[r,names(cont_raw[[1]][[r]][c])] = \n              ifelse(length(cont_raw[[1]][[r]][[c]][[1]])&gt;1,\n                     cont_raw[[1]][[r]][[c]][[1]][[1]],\n                     cont_raw[[1]][[r]][[c]][[1]])\n          },\n          error = function(e) print(paste(z, r, c, e)))\n      }\n    }\n  }\n  d = m %&gt;% data.frame() %&gt;% tibble()\n  \n  # debug\n  print(c(data %&gt;% dim))\n  \n  # merge database\n  if(z == 1)\n  {\n    data = d\n  }else\n  {\n    data[setdiff(names(d), names(data))] &lt;- NA\n    d[setdiff(names(d), names(data))] &lt;- NA\n    data = bind_rows(data, d)\n  }\n  \n  Sys.sleep(1.1)\n  \n}\n\nsaveRDS(data, \"data/data_TS_24_12\")\n\n\nIn order to avoid making further requests to the site, previously collected data are retrieved and the previous code is not executed.\n\n\nCode\ndata = readRDS(file = \"data/data_TS_24_12\")\ndata |&gt; \n  # reorder columns\n  select(\n    propertyType,\n    district,\n    price,\n    priceByArea,\n    size,\n    rooms,\n    bathrooms,\n    floor,\n    priceInfo,\n    newDevelopment,\n    parkingSpace,\n    parkingSpacePrice,\n    province,\n    municipality,\n    country,\n    operation,\n    latitude,\n    longitude,\n    status,\n    detailedType,\n    numPhotos\n  ) |&gt; \n  slice_sample(n = 30)"
  },
  {
    "objectID": "projects/2024-12-17_Idealista_API/index.html#clean-data",
    "href": "projects/2024-12-17_Idealista_API/index.html#clean-data",
    "title": "Idealista: housing sale in Trieste",
    "section": "Clean Data",
    "text": "Clean Data\nEasy and fast cleaning.\n\n\nCode\n# data.frame(1:dim(data)[2],data %&gt;% names)\n# str(data)\n# pulizia dei dati\ndata$floor[data$floor == \"bj\"] = 0\n\n# indexNumeric = c(1,4,5,6,9,11,12,18,19,20,23,29,30,43)\n\ndata %&gt;% \n  mutate_at(\n    vars(\n    price,\n    priceByArea,\n    parkingSpacePrice,\n    floor,\n    priceInfo,\n    size,\n    rooms,\n    bathrooms,\n    numPhotos,\n    parkingSpace,\n    latitude,\n    longitude,\n  ),\n  as.numeric\n  ) |&gt; \n  mutate_at(\n    vars(\n      propertyType,\n      operation,\n      province,\n      municipality,\n      district,\n      country,\n      status,\n      newDevelopment,\n      detailedType,\n      highlight,\n      typology,\n      subTypology,\n    ),\n    as.factor\n  ) |&gt; \n  mutate_at(\n    vars(\n      hasLift,\n      hasPlan,\n      has3DTour,\n      has360,\n      hasStaging,\n      hasVideo,\n      showAddress,\n      newDevelopmentFinished,\n      topNewDevelopment,\n      topPlus,\n      hasParkingSpace,\n      isParkingSpaceIncludedInPrice,\n      \n    ),\n    as.logical,\n  ) |&gt; \n  mutate(\n    across(district, \\(x) str_replace_all(x, \"-\", \" - \")),\n    city_area = if_else(is.na(neighborhood), district, neighborhood),\n    label = paste0(\n      \"Title: \", title, \"\\n\",\n      \"District: \", city_area, \"\\n\",\n      \"Floor: \", floor, \"\\n\",\n      \"Size: \", size, \" m^2\\n\",\n      \"Price: \", dollar(price, prefix = \"€\", suffix = \"k\", scale = .001), \"\\n\",\n      \"Price for m^2: \", dollar(priceByArea, prefix = \"€\"), \"\\n\",\n      \"Property type: \", propertyType, \"\\n\",\n      \"Rooms: \", rooms, \"\\n\",\n      \"Bathrooms: \", bathrooms, \"\\n\",\n      \"Status: \", status, \"\\n\"\n    ),\n  ) -&gt; data\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `floor = .Primitive(\"as.double\")(floor)`.\nCaused by warning:\n! NA introdotti per coercizione"
  },
  {
    "objectID": "projects/2024-12-17_Idealista_API/index.html#exploratory-data-analysis",
    "href": "projects/2024-12-17_Idealista_API/index.html#exploratory-data-analysis",
    "title": "Idealista: housing sale in Trieste",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIn this section, some plots are shown to give an idea of the data.\n\nPlots\n\n\nCode\ndata |&gt; \n  filter(!is.na(floor)) |&gt;\n  mutate(\n    across(city_area, \\(x) fct_na_value_to_level(x, \"NA\") |&gt; fct_lump_n(5))\n  ) |&gt; \n  summarise(\n    n = n(),\n    across(\n      priceByArea,\n      list(max = max, min = min, mean = mean),\n      .names =  \"{.col}_{.fn}\"\n      ),\n    .by = c(floor, city_area)\n  ) |&gt;\n  # transform n to a range between min and max of priceByArea\n  mutate(\n    across(\n      n,\n      \\(x) qunif(\n        (x - min(x))/(max(x) - min(x)),\n        priceByArea_min,\n        priceByArea_max\n        )\n      ),\n    .by = city_area\n  ) |&gt; \nggplot(aes(x = floor)) +\n  # just to have pricebyarea in the y axis\n  geom_line(\n    aes(y = priceByArea_mean),\n    alpha = 0,\n    ) +\n  # geom_ribbon(\n  #   aes(ymin = priceByArea_min, ymax = priceByArea_max),\n  #   fill = \"tomato\",\n  #   alpha = .3\n  #   ) +\n  geom_col(\n    aes(y = n),\n    alpha = .7,\n    fill = \"steelblue\",\n    ) +\n  geom_line(\n    aes(y = priceByArea_mean),\n    color = \"tomato\",\n    ) +\n  geom_ribbon(\n    aes(ymin = priceByArea_min, ymax = priceByArea_max),\n    fill = \"tomato\",\n    alpha = .3\n    ) +\n  facet_wrap(\n    ~city_area,\n    ncol = 3,\n    scales = \"free\",\n    labeller = label_wrap_gen()\n    ) +\n  scale_y_continuous(\n    name = \"Price for m^2\",\n    labels = ~ dollar(.x, prefix = \"€\"),\n    # limits = ~ list(0, max(.) * 1.1),\n    # sec.axis = sec_axis(trans = ~ . / max(.y), name = \"Price\")\n  ) +\n  scale_x_continuous(\n    labels = ~number(., accuracy = 1)\n  ) +\n  labs(\n    y = \"\",\n    title = \"Price for m^2 by floor and district\",\n    subtitle = \"Distribution of the price by m^2 in function of the floor of the houses in the different districts\",\n    \n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\np = data |&gt; \n  filter(!is.na(floor), !is.na(size)) |&gt; \n  mutate(\n    across(city_area, \\(x) fct_na_value_to_level(x, \"NA\") |&gt; fct_lump_n(8))\n  ) |&gt; \n  ggplot(aes(x = size, y = price, color = city_area,\n             group = city_area, text = label)) +\n  geom_point(\n    alpha = .7,\n    size = 1,\n  ) +\n  geom_smooth(\n    alpha = .9,\n    se = F,\n    linewidth = .5,\n    linetype = \"dashed\",\n  ) +\n  scale_y_log10(\n    labels = \\(x) dollar(x, prefix = \"€\", suffix = \"k\",\n                         scale = .001, accuracy = 1)\n  ) +\n  scale_x_continuous(\n    limits = c(15,200),\n    ) +\n  # scale_x_log10() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank(),\n  ) +\n  theme_minimal() +\n  guides(\n    color = guide_legend(nrow = 2)\n  ) +\n  labs(\n    x = \"Size (m^2)\",\n    y = \"Price\",\n    title = \"Price of the houses in relation to the size\",\n    color = \"District\"\n  )\n\n# interact the plot\nggplotly(p, tooltip = \"text\") |&gt; \n  plotly::layout(\n    width = 800,  \n    height = 750, \n    legend = list(\n      orientation = \"h\", \n      x = 0.5, \n      xanchor = \"center\", \n      y = -0.2\n    )\n  )\n\n\n\n\n\n\n\n\nMaps\nMappa del prezzo delle case nelle diverse zone della città. La mappa è interattiva, cliccando sui singoli pallini comparirà una box con ulteriori dati sulla casa.\n\n\n\nCode\npal = with(data, colorFactor(brewer.pal(10,\"RdYlGn\"), -priceByArea))\ndfPopup = data %&gt;% \n  mutate(popup_info = str_replace_all(label, \"\\n\", \"&lt;br&gt;\"))\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addCircleMarkers(data = dfPopup,\n                   lat = ~ latitude,\n                   lng = ~ longitude,\n                   radius = ~ 2,\n                   opacity = .7,\n                   color = ~ pal(-priceByArea),\n                   popup = ~ popup_info)\n\n\n\n\n\n\n\n\n\nCorrelation plot\n\n\nCode\ndata %&gt;% \n  select_if(is.numeric) %&gt;% \n  dplyr::select(\n    price,\n    size,\n    numPhotos,\n    floor,\n    rooms,\n    bathrooms,\n    ) %&gt;% \n  na.roughfix() %&gt;%  \n  cor %&gt;% \n  corrplot::corrplot(method = \"number\",\n                     hclust.method = \"ward.D2\",\n                     diag = F,\n                     type = \"upper\",\n                     order = \"hclust\",\n                     number.cex = .6)\n\n\n\n\n\n\n\n\n\n\n\nMissing data\nVisualize the features with NA and from that understand how to manipulate them.\n\n\nCode\nDataExplorer::plot_missing(data, missing_only = T, ggtheme = theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  select(\n    # select only columns with less than 80% of NAs\n    data |&gt; \n      summarise(\n        across(everything(), \\(x) sum(is.na(x)) / nrow(data))\n      ) |&gt; \n      pivot_longer(everything()) |&gt; \n      filter(value &lt; .8) |&gt; \n      pull(name)\n  ) -&gt; data"
  },
  {
    "objectID": "projects/2024-12-17_Idealista_API/index.html#model",
    "href": "projects/2024-12-17_Idealista_API/index.html#model",
    "title": "Idealista: housing sale in Trieste",
    "section": "Model",
    "text": "Model\n\nPre processing\nI develop an easy model to figure out how the variables for an ad influence the price posted on. I will use the {tidymodels} framework to deal it. The dataset will be splitted into 2 dataset, one for training and one for testing.\n\n\nCode\nset.seed(1)\n# data |&gt; glimpse()\ndata_split = initial_split(data, prop = .8, strata = city_area)\n\ndata_train = training(data_split)\ndata_test = testing(data_split)\n\n\nI use a penalized linear regression via {glmnet} package. I create a grid of parameters for penalty and mixture and I’ll train a model for each combination of parameters. After that I’ll estimate the metrics of the models and I’ll choice the best one based on RMSE (Root Mean Square Error).\n\n\nCode\n# set the model\nmod &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n  ) |&gt; \n  set_engine(\"glmnet\")\n\ndata_grid &lt;- grid_regular(penalty(c(5, 0)),\n                          mixture(c(0, 1)),\n                          levels = 30)\n\n# for the cross validation\nset.seed(1)\ndata_folds &lt;- vfold_cv(data_train, v = 10)\n\n\nIn the recipe, there is the formula and the pre process rules. I remove all columns with low variance and group less common levels of the factor variables. For the nominal predictors who are NA I assign the unknown category while for numeric ones I impute the median of the category.\n\n\nCode\n# set the recipe\nrec &lt;- recipe(price ~ ., data = data) |&gt; \n  # these items won't be bake. They could be useful for the future analysis\n  update_role(propertyCode, latitude, longitude, url, description, priceByArea,\n           title, label, new_role = \"ID\") |&gt; \n  # remove unuseful features\n  step_rm(thumbnail, priceInfo, distance, externalReference, subtitle,\n          neighborhood, district) |&gt; \n  # logical to factor\n  step_mutate_at(all_logical_predictors(), fn = ~ as.numeric(.)) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors()) |&gt; \n  # remove features almost equals\n  step_nzv(all_predictors(), freq_cut = 95/5) |&gt;\n  # for some levels who aren't present in training set but in testing set\n  step_novel(all_nominal_predictors()) |&gt; \n  # add unknown to missing values\n  step_unknown(all_nominal_predictors()) |&gt; \n  # group unfrequent classes to \"other\"\n  step_other(all_factor_predictors(), threshold = .1) |&gt;\n  # fill NAs who didn't manage them before\n  step_impute_median(all_numeric_predictors()) |&gt; \n  # NA omit\n  # step_naomit(all_predictors()) |&gt;\n  # step_dummy(all_logical_predictors(), one_hot = T) |&gt;   \n  step_dummy(all_nominal_predictors(), one_hot = T)  \n  # reduce multicollinearity\n  # step_corr(all_predictors(), threshold = .9, )\n\nworkflow() |&gt; \n  add_model(mod) |&gt; \n  add_recipe(rec) -&gt; wkflw\n\n# prep(rec, training = data_train) |&gt; \n#   bake(new_data = NULL)\n\nprep(rec, log_changes = T)\n\n\nstep_rm (rm_Hc7ab): \n removed (7): thumbnail, priceInfo, district, neighborhood, distance, ...\n\nstep_mutate_at (mutate_at_PL71w): same number of columns\n\nstep_zv (zv_OQv4A): \n removed (5): operation, province, country, topNewDevelopment, topPlus\n\nstep_nzv (nzv_ibiO2): \n removed (4): detailedType, has3DTour, has360, hasStaging\n\nstep_novel (novel_1EelD): same number of columns\n\nstep_unknown (unknown_lyXyh): same number of columns\n\nstep_other (other_V6alB): same number of columns\n\nstep_impute_median (impute_median_RlDvT): same number of columns\n\nstep_dummy (dummy_ZE3pu): \n new (24): propertyType_chalet, propertyType_flat, propertyType_other, ...\n removed (8): propertyType, address, municipality, status, ...\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 33\nID:         8\n\n\n\n\n\n── Training information \n\n\nTraining data contained 766 data points and 766 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variables removed: thumbnail, priceInfo, distance, ... | Trained\n\n\n• Variable mutation for: showAddress, hasVideo, hasPlan, ... | Trained\n\n\n• Zero variance filter removed: operation, province, country, ... | Trained\n\n\n• Sparse, unbalanced variable filter removed: detailedType, ... | Trained\n\n\n• Novel factor level assignment for: propertyType, address, ... | Trained\n\n\n• Unknown factor level assignment for: propertyType, address, ... | Trained\n\n\n• Collapsing factor levels for: propertyType, address, ... | Trained\n\n\n• Median imputation for: numPhotos, size, rooms, bathrooms, ... | Trained\n\n\n• Dummy variables from: propertyType, address, municipality, ... | Trained\n\n\n\n\nTuning\nIn the plots below there are the model metrics along different parameters (penalty and mixture).\n\n\nCode\n# Detect the number of cores and store in variable\ncores &lt;- detectCores() -1\ncl &lt;- makeCluster(cores)\nregisterDoParallel(cl)\n\npen_reg_res &lt;- \n  wkflw %&gt;% \n  tune_grid(\n    resamples =  data_folds,\n    grid = data_grid\n    )\n\nstopCluster(cl)\n\npen_reg_res |&gt; \n  collect_metrics() |&gt;\n  ggplot(aes(mixture, mean, color = penalty, group = penalty)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) \n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nCode\npen_reg_res |&gt; \n  collect_metrics() |&gt;\n  ggplot(aes(penalty, mean, color = mixture, group = mixture)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number())\n\n\n\n\n\n\n\n\n\nI’ll choise the best model based on RMSE and finalize the workflow.\n\n\nCode\nbest_mod &lt;- pen_reg_res |&gt; \n  select_best(\"rmse\")\n\nfinal_wf &lt;- finalize_workflow(wkflw, best_mod)\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n9 Recipe Steps\n\n• step_rm()\n• step_mutate_at()\n• step_zv()\n• step_nzv()\n• step_novel()\n• step_unknown()\n• step_other()\n• step_impute_median()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 6210.16941891562\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nFinal model\nThe final model has a RMSE of €31.850 and an \\(R^2\\) of 0.21.\n\n\nCode\nfinal_fitted &lt;- last_fit(final_wf, split = data_split)\n\n\nWarning: il pacchetto 'Matrix' è stato creato con R versione 4.3.3\n\n\nCode\nfinal_fitted |&gt; \n  collect_metrics() \n\n\n\n  \n\n\n\nI assume that the model could estimate the real price, so in the table below are shown the price published with the delta from the price predicted. This is useful to check when an house is over or under estimated. The model doesn’t take in account the whole parameters which an house agency should take, so that this model is just to give a first prediction of the house price.\n\n\nCode\npredictions &lt;- final_fitted |&gt; \n  collect_predictions() |&gt; \n  select(row = .row, price, predicted = .pred) |&gt; \n  mutate(\n    delta = price - predicted,\n    delta_percent = scales::percent(delta / predicted, accuracy = .1)\n  )\npredictions\n\n\n\n  \n\n\n\n\n\nCode\nplot &lt;-\n  predictions |&gt; \n  left_join(\n    data |&gt; mutate(row = 1:nrow(data)),\n    by = c(\"row\", \"price\")\n  ) |&gt; \n  # TODO: update label\n  ggplot(aes(price, predicted, text = label)) +\n  geom_point(aes(color = rank(-delta))) +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_text(aes(x = predictions$price[predictions$predicted&gt;1E6],\n  #               y = predictions$predicted[predictions$predicted&gt;1E6]), \n  #           label = \"99 Rooms\",\n  #           nudge_x = .2,\n  #           color = \"tomato\"\n  #           ) +\n  scale_x_log10(label = \\(x) scales::dollar(x, prefix = \"€\", big.mark = \".\")) +\n  scale_y_log10(label = \\(x) scales::dollar(x, prefix = \"€\", big.mark = \".\")) +\n  scale_color_continuous(low = \"tomato\", high = \"skyblue4\") +\n  labs(\n    x = \"Price posted on Idealista\",\n    y = \"Price predicted\",\n    title = \"Prediction VS Posted\"\n  )\nggplotly(plot, tooltip = c(\"text\", \"x\", \"y\"))\n\n\n\n\n\n\n\n\nCode\nfinal &lt;- fit(final_wf, data = data_train)\n\nfinal |&gt; \n  tidy() |&gt; \n  filter(abs(estimate) &lt; 1E5, abs(estimate) &gt; 1E1) |&gt; \n  ggplot(aes(estimate, term)) +\n  geom_col()"
  }
]